{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Simple-RL-Q-learning.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_5i1GSU0Oy6u",
        "colab_type": "text"
      },
      "source": [
        "# Lower, Higher or Same\n",
        "\n",
        "Agent will learn to guess if the next die would be lower than, higher than or as same as the current die.\n",
        "\n",
        "Applied from https://towardsdatascience.com/simple-reinforcement-learning-q-learning-fcddc4b6fe56\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-iLZxgzCOgcw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import random"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xPxZLS31On1L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# possible states of a 6 faces die \n",
        "possible_states = [0, 1, 2, 3, 4, 5]\n",
        "state_size = len(possible_states)\n",
        "\n",
        "# lo, hi, same\n",
        "possible_actions = [0, 1, 2]\n",
        "action_size = len(possible_actions)\n",
        "\n",
        "# Initialize q-table values to 0\n",
        "Q = np.zeros((state_size, action_size))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KV2-FAiiO4-u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set the percent you want to explore\n",
        "def pick_action(state, actions, Q, epsilon):\n",
        "  if random.uniform(0, 1) < epsilon:\n",
        "      \"\"\"\n",
        "      Explore: select a random action\n",
        "      \"\"\"\n",
        "      # action = actions[random.randint(0, len(actions)-1)]\n",
        "      action = random.randint(0, len(actions)-1)\n",
        "\n",
        "  else:\n",
        "      \"\"\"\n",
        "      Exploit: select the action with max value (future reward)\n",
        "      \"\"\"\n",
        "\n",
        "      action = np.argmax(Q, axis=1)[state]\n",
        "\n",
        "  return action"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DNEiC_vbO9vc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Update Q values\n",
        "def update_q(Q, cur_state, next_state, action, reward, gamma, lr):\n",
        "  Q[cur_state, action] = Q[cur_state, action] + lr * (reward + gamma * np.max(Q[next_state, :]) - Q[cur_state, action])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DyYffr9PTQ3T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Apply action to environment for reward\n",
        "def update(action, cur_state, special_rule = False):\n",
        "  # current die\n",
        "  cur_die = cur_state + 1\n",
        "\n",
        "  # roll the die\n",
        "  next_die = random.randint(1,state_size)\n",
        "\n",
        "  # check result\n",
        "  if next_die < cur_die and action == 0:\n",
        "    reward = 1\n",
        "  elif next_die > cur_die and action == 1:\n",
        "    reward = 1\n",
        "  elif next_die == cur_die and action == 2:\n",
        "    reward = 1\n",
        "  else:\n",
        "    reward = -1\n",
        "  \n",
        "  next_state = next_die - 1\n",
        "  \n",
        "  return next_state, reward\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2hXOUTGXPIUt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "episode_max = 10000\n",
        "cur_episode = 0\n",
        "\n",
        "gamma = 0.9 # discount factor for future reward\n",
        "cur_state = 0 # number 1\n",
        "epsilon = 0.5 # explore or exploit\n",
        "lr = 0.1 # learning rate\n",
        "\n",
        "while cur_episode < episode_max:\n",
        "\n",
        "  action = pick_action(cur_state, possible_actions, Q, epsilon)\n",
        "  next_state, reward = update(action, cur_state)\n",
        "  update_q(Q, cur_state, next_state, action, reward, gamma, lr)\n",
        "\n",
        "  cur_state = next_state\n",
        "  cur_episode += 1\n",
        "  epsilon *= 0.99\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0s7lGTvFVCS4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        },
        "outputId": "217b93ac-76f4-470f-d392-62f2d2998302"
      },
      "source": [
        "Q"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.1       ,  3.51083501, -0.23890985],\n",
              "       [-0.181171  ,  3.20086943, -0.14245432],\n",
              "       [-0.19405635,  2.89823776, -0.34692611],\n",
              "       [ 2.99464376, -0.28519206, -0.2181431 ],\n",
              "       [ 3.1378177 , -0.25716624, -0.22953163],\n",
              "       [ 3.80684366, -0.23370692,  0.06632637]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oOB1CVt3YDKe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "9e5c67dc-f61a-422e-fe9a-e191a996fa19"
      },
      "source": [
        "# test 1\n",
        "cur_state = random.randint(0, state_size-1)\n",
        "next_state = random.randint(0, state_size-1)\n",
        "\n",
        "action = pick_action(cur_state, possible_actions, Q, epsilon)\n",
        "possible_actions_txt = ['lower', 'higher', 'same']\n",
        "print('current number', cur_state+1)\n",
        "print('predict next:', possible_actions_txt[action])\n",
        "print('actual next number:', next_state+1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "current number 3\n",
            "predict next: higher\n",
            "actual next number: 3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wnuYV_Iga387",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e09e92a3-9377-4f22-fd10-a583a55feae9"
      },
      "source": [
        "# test batch\n",
        "test_num_max = 1000\n",
        "correct_num = 0\n",
        "epsilon = 0 # always exploit\n",
        "\n",
        "for i in range(test_num_max):\n",
        "  cur_state = random.randint(0, state_size-1)\n",
        "  action = pick_action(cur_state, possible_actions, Q, epsilon)\n",
        "  _, reward = update(action, cur_state)\n",
        "  if reward == 1:\n",
        "    correct_num += 1\n",
        "\n",
        "print('accuracy:', correct_num/test_num_max)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accuracy: 0.685\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}